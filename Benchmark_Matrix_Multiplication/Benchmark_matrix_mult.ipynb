{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \n",
      "[GCC 7.3.0]\n",
      "Numpy version: 1.16.0\n",
      "Pandas version: 0.23.4\n",
      "PyTorch version: 1.0.0\n",
      "Pyculib version: 1.0.2+7.g9744803\n",
      "BLAS info:\n",
      "blas_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "blis_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "blas_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_lapack_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "None\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from numba import vectorize, cuda\n",
    "from numba.cuda.cudadrv.error import CudaDriverError\n",
    "import scipy.linalg.blas as blas\n",
    "import pyculib.blas as cublas\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyculib\n",
    "from pyculib.blas import Blas\n",
    "from utils import (get_number_processors, get_ram_memory, get_total_gpu_memory, \n",
    "                   get_gpu_name, get_cuda_version, get_cudnn_version, AttributeDict,\n",
    "                   get_object_size, clear_memory_all_gpus)\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Numpy version: {}\".format(np.__version__))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "print(\"Pyculib version: {}\".format(pyculib.__version__))\n",
    "print(\"BLAS info:\") \n",
    "print(np.show_config())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2],[3,4]],dtype=np.float32)\n",
    "b=np.array([[1,1],[2,2]],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_matmul(a,b):\n",
    "    return np.dot(a,b)\n",
    "\n",
    "def pytorch_matmul(a,b):\n",
    "    at = torch.as_tensor(a).cuda() \n",
    "    bt = torch.as_tensor(b).cuda()\n",
    "    return torch.mm(at,bt)\n",
    "\n",
    "def pyculib_matmul(a, b):\n",
    "    A_d = cuda.to_device(a)\n",
    "    B_d = cuda.to_device(b)\n",
    "    return cublas.gemm(\"N\", \"N\", 1.0, A_d, B_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  5.],\n",
       "       [11., 11.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  5.],\n",
       "        [11., 11.]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  5.],\n",
       "       [11., 11.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyculib_matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36082494 -0.32567266  0.02785533]\n",
      " [ 3.0361838   1.5626514   4.9736047 ]\n",
      " [-1.3531643  -0.5507817  -3.2932088 ]]\n",
      "[[-0.36082489 -0.32567268  0.02785538]\n",
      " [ 3.03618366  1.56265139  4.97360477]\n",
      " [-1.35316433 -0.55078167 -3.29320896]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "A = np.random.randn(3, 3)\n",
    "B = np.random.randn(3, 3)\n",
    "\n",
    "C = blas.sgemm(1.0, A, B)\n",
    "print(C)\n",
    "\n",
    "A_d = cuda.to_device(A)\n",
    "B_d = cuda.to_device(B)\n",
    "\n",
    "C_d = cublas.gemm(\"N\", \"N\", 1.0, A_d, B_d)\n",
    "print(C_d)\n",
    "#C_h = np.zeros((3, 3), dtype=np.float64)\n",
    "#C_d.copy_to_host(C_h)\n",
    "#print(C_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nyu-cds.github.io/python-numba/05-cuda/\n",
    "#https://stackoverflow.com/questions/36526708/comparing-python-numpy-numba-and-c-for-matrix-multiplication\n",
    "#http://jiajiamomomo.blogspot.com/2017/04/running-numba-example-of-matrix.html\n",
    "#http://numba.pydata.org/numba-doc/0.17.0/cuda/examples.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (benchmark)",
   "language": "python",
   "name": "benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
