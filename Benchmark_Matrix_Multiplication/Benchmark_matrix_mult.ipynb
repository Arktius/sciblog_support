{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skcuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-468e906cdbd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyculib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m from utils import (get_number_processors, get_ram_memory, get_total_gpu_memory, \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skcuda'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from numba import vectorize, cuda\n",
    "from numba.cuda.cudadrv.error import CudaDriverError\n",
    "import scipy.linalg.blas as blas\n",
    "import pyculib.blas as cublas\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import skcuda.linalg as linalg\n",
    "import pyculib\n",
    "from utils import (get_number_processors, get_ram_memory, get_total_gpu_memory, \n",
    "                   get_gpu_name, get_cuda_version, get_cudnn_version, AttributeDict,\n",
    "                   get_object_size, clear_memory_all_gpus)\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Numpy version: {}\".format(np.__version__))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "print(\"Pyculib version: {}\".format(pyculib.__version__))\n",
    "print(\"BLAS info:\") \n",
    "print(np.show_config())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2],[3,4]],dtype=np.float32)\n",
    "b=np.array([[1,1],[2,2]],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_matmul(a,b):\n",
    "    return np.dot(a,b)\n",
    "\n",
    "def scipy_matmul(a,b):\n",
    "    return blas.sgemm(1.0,a,b)\n",
    "\n",
    "def pytorch_matmul(a,b):\n",
    "    at = torch.as_tensor(a).cuda() \n",
    "    bt = torch.as_tensor(b).cuda()\n",
    "    return torch.mm(at,bt)\n",
    "\n",
    "def pyculib_matmul(a, b):\n",
    "    A_d = cuda.to_device(a)\n",
    "    B_d = cuda.to_device(b)\n",
    "    return cublas.gemm(\"N\", \"N\", 1.0, A_d, B_d)\n",
    "\n",
    "def scikit_cuda_matmul(a,b):\n",
    "    x_gpu = gpuarray.to_gpu(a)\n",
    "    y_gpu = gpuarray.to_gpu(b)\n",
    "    return linalg.multiply(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  5.],\n",
       "       [11., 11.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.,   5.],\n",
       "       [ 11.,  11.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy_matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  5.],\n",
       "        [11., 11.]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  5.],\n",
       "       [11., 11.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyculib_matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linalg.init()\n",
    "scikit_cuda_matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36082494 -0.32567266  0.02785533]\n",
      " [ 3.0361838   1.5626514   4.9736047 ]\n",
      " [-1.3531643  -0.5507817  -3.2932088 ]]\n",
      "[[-0.36082489 -0.32567268  0.02785538]\n",
      " [ 3.03618366  1.56265139  4.97360477]\n",
      " [-1.35316433 -0.55078167 -3.29320896]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "A = np.random.randn(3, 3)\n",
    "B = np.random.randn(3, 3)\n",
    "\n",
    "C = blas.sgemm(1.0, A, B)\n",
    "print(C)\n",
    "\n",
    "A_d = cuda.to_device(A)\n",
    "B_d = cuda.to_device(B)\n",
    "\n",
    "C_d = cublas.gemm(\"N\", \"N\", 1.0, A_d, B_d)\n",
    "print(C_d)\n",
    "#C_h = np.zeros((3, 3), dtype=np.float64)\n",
    "#C_d.copy_to_host(C_h)\n",
    "#print(C_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nyu-cds.github.io/python-numba/05-cuda/\n",
    "#https://stackoverflow.com/questions/36526708/comparing-python-numpy-numba-and-c-for-matrix-multiplication\n",
    "#http://jiajiamomomo.blogspot.com/2017/04/running-numba-example-of-matrix.html\n",
    "#http://numba.pydata.org/numba-doc/0.17.0/cuda/examples.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bench)",
   "language": "python",
   "name": "benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
