{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark of Matrix Multiplications\n",
    "On this benchmark we compare several operations using numpy, numexpr and numba (CPU&GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \n",
      "[GCC 7.3.0]\n",
      "Numpy version: 1.15.4\n",
      "Pandas version: 0.23.4\n",
      "Numexpr version: 2.6.9\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import numexpr as ne\n",
    "from numba import vectorize\n",
    "import math\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import bokeh\n",
    "from utils import (get_number_processors, get_ram_memory, get_total_gpu_memory, \n",
    "                   get_gpu_name, get_cuda_version, get_cudnn_version, AttributeDict)\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Numpy version: {}\".format(np.__version__))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Numexpr version: {}\".format(ne.__version__))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a,b):\n",
    "    return a*b\n",
    "\n",
    "def exponential(a, b):\n",
    "    return a*np.exp(b)\n",
    "\n",
    "def sine(a, b):\n",
    "    return a*np.sin(b)\n",
    "\n",
    "# A general function that multiplies an arbitrary number of matrices\n",
    "# is 28% slower than directly multiplying the factors.\n",
    "# The function multiply_list is not used, just leaving it here for reference\n",
    "def multiply_list(l):\n",
    "    return reduce(lambda x, y: x*y, l) \n",
    "\n",
    "def multiply3(a, b, c):\n",
    "    return a*b*c\n",
    "\n",
    "def multiply5(a, b, c, d, e):\n",
    "    return a*b*c*d*e\n",
    "\n",
    "def exponential_sine(a, b, c):\n",
    "    return a*np.exp(b)*np.sin(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function with numpexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ne_multiply(a,b):\n",
    "    return ne.evaluate(\"a*b\")\n",
    "\n",
    "def ne_exponential(a, b):\n",
    "    return ne.evaluate(\"a*exp(b)\")\n",
    "\n",
    "def ne_sine(a, b):\n",
    "    return ne.evaluate(\"a*sin(b)\")\n",
    "\n",
    "def ne_multiply3(a, b, c):\n",
    "    return ne.evaluate(\"a*b*c\")\n",
    "\n",
    "def ne_multiply5(a, b, c, d, e):\n",
    "    return ne.evaluate(\"a*b*c*d*e\")\n",
    "\n",
    "def ne_exponential_sine(a, b, c):\n",
    "    return ne.evaluate(\"a*exp(b)*sin(c)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for numba\n",
    "NOTE: For numba solutions, having a solution empty vector speeds up around 10%\n",
    "```\n",
    "r0 = np.empty((S1, S2), dtype=np.int16)\n",
    "r0 = multicpu(a, b)\n",
    "```\n",
    "source: https://devblogs.nvidia.com/numba-python-cuda-acceleration/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize([\"int16(int16, int16)\"], target=\"cpu\")\n",
    "def multicpu(a, b):\n",
    "    return a * b\n",
    "\n",
    "@vectorize([\"int16(int16, int16)\"], target=\"cuda\")\n",
    "def multicuda(a, b):\n",
    "    return a * b\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target=\"cpu\")\n",
    "def multfcpu(a, b):\n",
    "    return a * b\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target=\"cuda\")\n",
    "def multfcuda(a, b):\n",
    "    return a * b\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target=\"cpu\")\n",
    "def expcpu(a, b):\n",
    "    return a*math.exp(b)\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target=\"cuda\")\n",
    "def expcuda(a, b):\n",
    "    return a*math.exp(b)\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target=\"cpu\")\n",
    "def sincpu(a, b):\n",
    "    return a*math.sin(b)\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target=\"cuda\")\n",
    "def sincuda(a, b):\n",
    "    return a*math.sin(b)\n",
    "\n",
    "@vectorize([\"float32(float32, float32, float32)\"], target=\"cpu\")\n",
    "def multfcpu3(a, b, c):\n",
    "    return a * b * c\n",
    "\n",
    "@vectorize([\"float32(float32, float32, float32)\"], target=\"cuda\")\n",
    "def multfcuda3(a, b, c):\n",
    "    return a * b * c\n",
    "\n",
    "@vectorize([\"float32(float32, float32, float32, float32, float32)\"], target=\"cpu\")\n",
    "def multfcpu5(a, b, c, d, e):\n",
    "    return a * b * c * d * e\n",
    "\n",
    "@vectorize([\"float32(float32, float32, float32, float32, float32)\"], target=\"cuda\")\n",
    "def multfcuda5(a, b, c, d, e):\n",
    "    return a * b * c * d * e\n",
    "\n",
    "@vectorize([\"float32(float32, float32, float32)\"], target=\"cpu\")\n",
    "def expsincpu(a, b, c):\n",
    "    return a*math.exp(b)*math.sin(c)\n",
    "\n",
    "@vectorize([\"float32(float32, float32, float32)\"], target=\"cuda\")\n",
    "def expsincuda(a, b, c):\n",
    "    return a*math.exp(b)*math.sin(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factors_int(s1=100, s2=100):\n",
    "    a = np.random.randint(1, 5, (s1, s2), dtype=np.int16)\n",
    "    b = np.random.randint(1, 10, (s1, s2), dtype=np.int16)\n",
    "    return a, b\n",
    "\n",
    "def factors_float(s1=100, s2=100):\n",
    "    a = np.random.randn(s1, s2).astype(np.float32)\n",
    "    b = np.random.randn(s1, s2).astype(np.float32)\n",
    "    return a, b\n",
    "\n",
    "def factors_float3(s1=100, s2=100):\n",
    "    a = np.random.randn(s1, s2).astype(np.float32)\n",
    "    b = np.random.randn(s1, s2).astype(np.float32)\n",
    "    c = np.random.uniform(low=0, high=10, size=(s1,s2))\n",
    "    return a, b, c\n",
    "\n",
    "def factors_float5(s1=100, s2=100):\n",
    "    a = np.random.randn(s1, s2).astype(np.float32)\n",
    "    b = np.random.randn(s1, s2).astype(np.float32)\n",
    "    c = np.random.uniform(low=0, high=10, size=(s1,s2))\n",
    "    d = np.random.uniform(low=5, high=15, size=(s1,s2))\n",
    "    e = np.random.uniform(low=0, high=30, size=(s1,s2))\n",
    "    return a, b, c, d, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_combinations=[\n",
    "    (100, 100),\n",
    "    (1000, 1000),\n",
    "    (10000, 10000),\n",
    "    (100000, 10000),\n",
    "    (100000, 100000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"n_processors\",\n",
    "           \"cpu_memory\",\n",
    "           \"gpu_name\",\n",
    "           \"gpu_memory\",\n",
    "           \"data_type\",\n",
    "           \"size1\",\n",
    "           \"size2\",\n",
    "           \"operation\",\n",
    "           \"numpy\",\n",
    "           \"numexpr\",\n",
    "           \"numba_cpu\",\n",
    "           \"numba_gpu\"]\n",
    "df = pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_processors = get_number_processors()\n",
    "cpu_memory = get_ram_memory(units=\"Gb\")\n",
    "gpu_name = get_gpu_name()[0]\n",
    "gpu_memory = get_total_gpu_memory(units=\"Gb\")[0]\n",
    "header = [n_processors, cpu_memory, gpu_name, gpu_memory]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15 µs ± 19.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "460 µs ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "2.08 µs ± 25.6 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "1.46 ms ± 16.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "214 µs ± 3.11 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "588 µs ± 13.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "211 µs ± 703 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "4.15 ms ± 61.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "87.5 ms ± 541 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "21.3 ms ± 402 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "87.2 ms ± 541 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "134 ms ± 2.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "875 ms ± 1.57 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "180 ms ± 2.77 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "871 ms ± 5.73 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.29 s ± 48.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.81 s ± 21.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.74 s ± 24 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.8 s ± 32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "OOM for size (100000,100000)\n"
     ]
    }
   ],
   "source": [
    "for s1, s2 in size_combinations:\n",
    "    a, b = factors_int(s1, s2)\n",
    "    operation = \"a * b\"\n",
    "    r1 = %timeit -o multiply(a,b)\n",
    "    r2 = %timeit -o ne_multiply(a,b)\n",
    "    r3 = %timeit -o multicpu(a,b)\n",
    "    try:\n",
    "        r4 = %timeit -o multicuda(a,b)\n",
    "    except: # in case of Out Of Memory (OOM)\n",
    "        r4 = AttributeDict()\n",
    "        r4[\"average\"] = \"OOM\"\n",
    "        print(\"OOM for size ({},{})\".format(s1, s2))\n",
    "    row = header + [type(a[0,0]), s1, s2, operation, r1.average, r2.average, r3.average, r4.average]\n",
    "    df.loc[len(df)] = row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04 µs ± 18.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "455 µs ± 11.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "3.33 µs ± 5.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "1.35 ms ± 5.95 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "42.6 µs ± 1.69 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "576 µs ± 17 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "423 µs ± 2.26 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "4.81 ms ± 32.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "24.5 ms ± 1.25 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "31.8 ms ± 242 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "177 ms ± 413 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "262 ms ± 36.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "for s1, s2 in size_combinations:\n",
    "    a, b = factors_float(s1, s2)\n",
    "    operation = \"a * b\"\n",
    "    r1 = %timeit -o multiply(a,b)\n",
    "    r2 = %timeit -o ne_multiply(a,b)\n",
    "    r3 = %timeit -o multfcpu(a,b)\n",
    "    try:\n",
    "        r4 = %timeit -o multfcuda(a,b)\n",
    "    except: # in case of Out Of Memory (OOM)\n",
    "        r4 = AttributeDict()\n",
    "        r4[\"average\"] = \"OOM\"\n",
    "        print(\"OOM for size ({},{})\".format(s1, s2))\n",
    "    row = header + [type(a[0,0]), s1, s2, operation, r1.average, r2.average, r3.average, r4.average]\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04 µs ± 19.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "451 µs ± 16.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "3.39 µs ± 29.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "1.44 ms ± 6.81 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "41.7 µs ± 1.04 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "562 µs ± 10.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "426 µs ± 6.44 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "4.76 ms ± 70.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "23.9 ms ± 511 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "31.9 ms ± 205 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "175 ms ± 834 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "256 ms ± 37.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "231 ms ± 11.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "270 ms ± 7.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.81 s ± 14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "2.65 s ± 62.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "2.12 s ± 139 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "2.42 s ± 65.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "20.5 s ± 5.02 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "OOM for size (100000,100000)\n"
     ]
    }
   ],
   "source": [
    "for s1, s2 in size_combinations:\n",
    "    a, b = factors_float(s1, s2)\n",
    "    operation = \"a * exp(b)\"\n",
    "    r1 = %timeit -o multiply(a,b)\n",
    "    r2 = %timeit -o ne_multiply(a,b)\n",
    "    r3 = %timeit -o multfcpu(a,b)\n",
    "    try:\n",
    "        r4 = %timeit -o multfcuda(a,b)\n",
    "    except: # in case of Out Of Memory (OOM)\n",
    "        r4 = AttributeDict()\n",
    "        r4[\"average\"] = \"OOM\"\n",
    "        print(\"OOM for size ({},{})\".format(s1, s2))        \n",
    "    row = header + [type(a[0,0]), s1, s2, operation, r1.average, r2.average, r3.average, r4.average]\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sine matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s1, s2 in size_combinations:\n",
    "    a, b = factors_float(s1, s2)\n",
    "    operation = \"a * sin(b)\"\n",
    "    r1 = %timeit -o multiply(a,b)\n",
    "    r2 = %timeit -o ne_multiply(a,b)\n",
    "    r3 = %timeit -o multfcpu(a,b)\n",
    "    try:\n",
    "        r4 = %timeit -o multfcuda(a,b)\n",
    "    except: # in case of Out Of Memory (OOM)\n",
    "        r4 = AttributeDict()\n",
    "        r4[\"average\"] = \"OOM\"\n",
    "        print(\"OOM for size ({},{})\".format(s1, s2))        \n",
    "    row = header + [type(a[0,0]), s1, s2, operation, r1.average, r2.average, r3.average, r4.average]\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple matrix multiplication (3 factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s1, s2 in size_combinations:\n",
    "    a, b, c = factors_float3(s1, s2)\n",
    "    operation = \"a * b * c\"\n",
    "    r1 = %timeit -o multiply3(a,b,c)\n",
    "    r2 = %timeit -o ne_multiply3(a,b,c)\n",
    "    r3 = %timeit -o multfcpu3(a,b,c)\n",
    "    try:\n",
    "        r4 = %timeit -o multfcuda3(a,b,c)\n",
    "    except: # in case of Out Of Memory (OOM)\n",
    "        r4 = AttributeDict()\n",
    "        r4[\"average\"] = \"OOM\"\n",
    "        print(\"OOM for size ({},{})\".format(s1, s2))\n",
    "    row = header + [type(a[0,0]), s1, s2, operation, r1.average, r2.average, r3.average, r4.average]\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple matrix multiplication (5 factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s1, s2 in size_combinations:\n",
    "    a, b, c, d, e = factors_float5(s1, s2)\n",
    "    operation = \"a * b * c * d * e\"\n",
    "    r1 = %timeit -o multiply5(a,b,c,d,e)\n",
    "    r2 = %timeit -o ne_multiply5(a,b,c,d,e)\n",
    "    r3 = %timeit -o multfcpu5(a,b,c,d,e)\n",
    "    try:\n",
    "        r4 = %timeit -o multfcuda5(a,b,c,d,e)\n",
    "    except: # in case of Out Of Memory (OOM)\n",
    "        r4 = AttributeDict()\n",
    "        r4[\"average\"] = \"OOM\"\n",
    "        print(\"OOM for size ({},{})\".format(s1, s2))\n",
    "    row = header + [type(a[0,0]), s1, s2, operation, r1.average, r2.average, r3.average, r4.average]\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential sine matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s1, s2 in size_combinations:\n",
    "    a, b, c = factors_float3(s1, s2)\n",
    "    operation = \"a * exp(b) * sin(c)\"\n",
    "    r1 = %timeit -o exponential_sine(a,b,c)\n",
    "    r2 = %timeit -o ne_exponential_sine(a,b,c)\n",
    "    r3 = %timeit -o expsincpu(a,b,c)\n",
    "    try:\n",
    "        r4 = %timeit -o expsincuda(a,b,c)\n",
    "    except: # in case of Out Of Memory (OOM)\n",
    "        r4 = AttributeDict()\n",
    "        r4[\"average\"] = \"OOM\"\n",
    "        print(\"OOM for size ({},{})\".format(s1, s2))        \n",
    "    row = header + [type(a[0,0]), s1, s2, operation, r1.average, r2.average, r3.average, r4.average]\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_processors</th>\n",
       "      <th>cpu_memory</th>\n",
       "      <th>gpu_name</th>\n",
       "      <th>gpu_memory</th>\n",
       "      <th>data_type</th>\n",
       "      <th>size1</th>\n",
       "      <th>size2</th>\n",
       "      <th>operation</th>\n",
       "      <th>numpy</th>\n",
       "      <th>numexpr</th>\n",
       "      <th>numba_cpu</th>\n",
       "      <th>numba_gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00140176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.00459222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.087743</td>\n",
       "      <td>0.021155</td>\n",
       "      <td>0.088151</td>\n",
       "      <td>0.145024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.888377</td>\n",
       "      <td>0.168696</td>\n",
       "      <td>0.919646</td>\n",
       "      <td>1.45222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00138766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.00455572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.089223</td>\n",
       "      <td>0.021309</td>\n",
       "      <td>0.087749</td>\n",
       "      <td>0.143423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.880225</td>\n",
       "      <td>0.171586</td>\n",
       "      <td>0.869740</td>\n",
       "      <td>1.29224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0014146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.00428717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.087642</td>\n",
       "      <td>0.021270</td>\n",
       "      <td>0.087442</td>\n",
       "      <td>0.134811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.880368</td>\n",
       "      <td>0.167730</td>\n",
       "      <td>0.894005</td>\n",
       "      <td>1.29701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00137754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.0041702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.087442</td>\n",
       "      <td>0.021414</td>\n",
       "      <td>0.087755</td>\n",
       "      <td>0.135153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.886680</td>\n",
       "      <td>0.178111</td>\n",
       "      <td>0.877102</td>\n",
       "      <td>1.29477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00139816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.00414853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.087256</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>0.087827</td>\n",
       "      <td>0.135278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100000</td>\n",
       "      <td>10000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>0.886008</td>\n",
       "      <td>0.178599</td>\n",
       "      <td>0.867855</td>\n",
       "      <td>1.2819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24</td>\n",
       "      <td>440.909752</td>\n",
       "      <td>Tesla V100-PCIE-16GB</td>\n",
       "      <td>15.781738</td>\n",
       "      <td>&lt;class 'numpy.int16'&gt;</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>a * b</td>\n",
       "      <td>8.830945</td>\n",
       "      <td>1.706681</td>\n",
       "      <td>8.782212</td>\n",
       "      <td>OOM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_processors  cpu_memory              gpu_name  gpu_memory  \\\n",
       "0            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "1            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "2            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "3            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "4            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "5            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "6            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "7            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "8            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "9            24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "10           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "11           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "12           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "13           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "14           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "15           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "16           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "17           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "18           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "19           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "20           24  440.909752  Tesla V100-PCIE-16GB   15.781738   \n",
       "\n",
       "                data_type   size1   size2 operation     numpy   numexpr  \\\n",
       "0   <class 'numpy.int16'>     100     100     a * b  0.000002  0.000467   \n",
       "1   <class 'numpy.int16'>    1000    1000     a * b  0.000213  0.000576   \n",
       "2   <class 'numpy.int16'>   10000   10000     a * b  0.087743  0.021155   \n",
       "3   <class 'numpy.int16'>  100000   10000     a * b  0.888377  0.168696   \n",
       "4   <class 'numpy.int16'>     100     100     a * b  0.000002  0.000466   \n",
       "5   <class 'numpy.int16'>    1000    1000     a * b  0.000212  0.000579   \n",
       "6   <class 'numpy.int16'>   10000   10000     a * b  0.089223  0.021309   \n",
       "7   <class 'numpy.int16'>  100000   10000     a * b  0.880225  0.171586   \n",
       "8   <class 'numpy.int16'>     100     100     a * b  0.000002  0.000462   \n",
       "9   <class 'numpy.int16'>    1000    1000     a * b  0.000212  0.000582   \n",
       "10  <class 'numpy.int16'>   10000   10000     a * b  0.087642  0.021270   \n",
       "11  <class 'numpy.int16'>  100000   10000     a * b  0.880368  0.167730   \n",
       "12  <class 'numpy.int16'>     100     100     a * b  0.000002  0.000457   \n",
       "13  <class 'numpy.int16'>    1000    1000     a * b  0.000210  0.000591   \n",
       "14  <class 'numpy.int16'>   10000   10000     a * b  0.087442  0.021414   \n",
       "15  <class 'numpy.int16'>  100000   10000     a * b  0.886680  0.178111   \n",
       "16  <class 'numpy.int16'>     100     100     a * b  0.000002  0.000454   \n",
       "17  <class 'numpy.int16'>    1000    1000     a * b  0.000213  0.000573   \n",
       "18  <class 'numpy.int16'>   10000   10000     a * b  0.087256  0.021168   \n",
       "19  <class 'numpy.int16'>  100000   10000     a * b  0.886008  0.178599   \n",
       "20  <class 'numpy.int16'>  100000  100000     a * b  8.830945  1.706681   \n",
       "\n",
       "    numba_cpu   numba_gpu  \n",
       "0    0.000002  0.00140176  \n",
       "1    0.000212  0.00459222  \n",
       "2    0.088151    0.145024  \n",
       "3    0.919646     1.45222  \n",
       "4    0.000002  0.00138766  \n",
       "5    0.000211  0.00455572  \n",
       "6    0.087749    0.143423  \n",
       "7    0.869740     1.29224  \n",
       "8    0.000002   0.0014146  \n",
       "9    0.000212  0.00428717  \n",
       "10   0.087442    0.134811  \n",
       "11   0.894005     1.29701  \n",
       "12   0.000002  0.00137754  \n",
       "13   0.000211   0.0041702  \n",
       "14   0.087755    0.135153  \n",
       "15   0.877102     1.29477  \n",
       "16   0.000002  0.00139816  \n",
       "17   0.000212  0.00414853  \n",
       "18   0.087827    0.135278  \n",
       "19   0.867855      1.2819  \n",
       "20   8.782212         OOM  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"benchmark_V10016GB_mult_exp_sin_100000x100000.csv\" \n",
    "#df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (benchmark)",
   "language": "python",
   "name": "benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
